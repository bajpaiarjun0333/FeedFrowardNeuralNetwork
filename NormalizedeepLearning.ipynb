{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQBMYJz3azNs"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset loading \n",
        "from keras.datasets import fashion_mnist\n",
        "#global x_train\n",
        "#global y_train\n",
        "#global x_test\n",
        "#global y_test\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train=x_train.reshape(x_train.shape[0],(x_train.shape[1]*x_train.shape[2]))\n",
        "x_test=x_test.reshape(x_test.shape[0],(x_test.shape[1]*x_test.shape[2]))\n",
        "x_train=x_train/255\n",
        "x_test=x_test/255\n"
      ],
      "metadata": {
        "id": "MkUeLRnIbPay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15ed5fbd-1b7c-45b8-c703-51bf1703c41e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ab chalu karte hain kuch main kaam kaaj\n",
        "#apne ko 3 cheez ki zarurat padegi \n",
        "#weight store karne ke liye ek 3 dimensional matrix(l,m,n)\n",
        "#bias store krane ke liye matrix(l*bias)\n",
        "#activation store krane ke liye list chaiye of matrix(l,m,n)\n",
        "#postactivation store karwane ke liye phir chaiye rhega\n",
        "#when i make a layer then it will give me the corresponding weight matrix, bias term \n",
        "#forward pass ka ek function likhna padega \n",
        "#backward pass ka alag function likhna pdega"
      ],
      "metadata": {
        "id": "-wmIYqqMdLgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  #shape of x will be n*k w will be k*m so x*w will give us n*m and b will be 1*m so using vectorization we get n*m finally \n",
        "  #z=np.matmul(x,w) # dimension has become n*m\n",
        "  #z=np.add(z,b)\n",
        "  for i in range(z.shape[0]):\n",
        "    idx=np.argmax(z[i])\n",
        "    #idx2=np.argmin(z[i])\n",
        "  #print(z)\n",
        "  return 1/(1+np.exp(-z))"
      ],
      "metadata": {
        "id": "dEe8fp-dg_M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_derivative(z):\n",
        "  return np.multiply((1/(1+np.exp(-z))),(1-(1/(1+np.exp(-z)))))"
      ],
      "metadata": {
        "id": "vOiJGgIoiQhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  #x=np.copy(z)\n",
        "  for i in range(x.shape[0]):\n",
        "    sum=0\n",
        "    #minidx=np.argmin(x[i])\n",
        "    #maxidx=np.argmax(x[i])\n",
        "    #x[i]=(x[i]-x[i][minidx])/(x[i][maxidx]-x[i][minidx])\n",
        "    for j in range(x.shape[1]):\n",
        "      sum=sum+np.exp(x[i][j])\n",
        "    x[i]=np.exp(x[i])/sum\n",
        "  return x"
      ],
      "metadata": {
        "id": "SggmTt9P-Quj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def softmax_helper(z):\n",
        "#     assert len(z.shape) == 2\n",
        "#     s = np.max(z, axis=1)\n",
        "#     s = s[:, np.newaxis] # necessary step to do broadcasting\n",
        "#     e_x = np.exp(z - s)\n",
        "#     div = np.sum(e_x, axis=1)\n",
        "#     div = div[:, np.newaxis] # dito\n",
        "#     return e_x / div"
      ],
      "metadata": {
        "id": "XDC7eU2LIT39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(yhat,y_train):\n",
        "  #chalo game shuru karte hain\n",
        "  #yhat n*10\n",
        "  sum=0 \n",
        "  for i in range(y_train.shape[0]):\n",
        "    sum+=-((np.log2(yhat[i][y_train[i]])))\n",
        "  return sum"
      ],
      "metadata": {
        "id": "DI83p5NGBbmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_layer(w,b,input,output):\n",
        "  weights=np.random.uniform(-1,1,(input,output))\n",
        "  bias=np.random.uniform(-1,1,(1,output))\n",
        "  w.append(weights)\n",
        "  b.append(bias)\n",
        "  return (w,b)"
      ],
      "metadata": {
        "id": "gYUxQUVmjOw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(x,w,b,a,h):\n",
        "  l=len(w)\n",
        "  #use a for loop for the required functionality\n",
        "  for i in range (l-1):\n",
        "    a1=np.add(np.matmul(x,w[i]),b[i])\n",
        "    h1=sigmoid(a1)\n",
        "    a.append(a1)\n",
        "    h.append(h1)\n",
        "    x=h1\n",
        "  #this loop will complete the forward pass\n",
        "  #for the last layer\n",
        "  a1=np.add(np.matmul(x,w[l-1]),b[l-1])\n",
        "  h1=softmax(a1)\n",
        "  a.append(a1)\n",
        "  h.append(h1)\n",
        "  return (a,h)"
      ],
      "metadata": {
        "id": "vo2s3Q8xg7Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_pass(wd,bd,ad,hd,w,b,a,h,yhat,y_train,x_train,no_of_classes):\n",
        "  #yahan par derivative ulte calculate honge yeh yaad rakhna,to saare loop ulte chalane honge humko\n",
        "  #apne ko yl banana padega for all the n inputs\n",
        "  #last layer ke liye special treatment\n",
        "  el=np.zeros((y_train.shape[0],no_of_classes))\n",
        "  for i in range (y_train.shape[0]):\n",
        "      el[i][y_train[i]]=1\n",
        "\n",
        "  yhatl=np.zeros((yhat.shape[0],1))\n",
        "  for i in range (yhat.shape[0]):\n",
        "    yhatl[i]=yhat[i][y_train[i]]\n",
        "  #calculate for the last layer\n",
        "  hd1=-(el/yhatl)\n",
        "  ad1=-(el-yhat)\n",
        "  hd.append(hd1)\n",
        "  ad.append(ad1)\n",
        "  #ab loop me daal kar saare ke saare humesha last appended wala hi use hoga\n",
        "  l=len(w)\n",
        "  for i in range(l-1,-1,-1):\n",
        "    # print(\"i \"+str(i))\n",
        "    # print(h[i-1].shape)\n",
        "    #print(ad[len(ad)-1].shape)\n",
        "    q=h[i-1].T\n",
        "    if i==0:\n",
        "      q=x_train.T\n",
        "    wi=np.matmul(q,ad[len(ad)-1])/x_train.shape[0]\n",
        "    bi=np.sum(ad[len(ad)-1],axis=0)/x_train.shape[0]\n",
        "    if i!=0:\n",
        "      hd1=np.matmul(ad[len(ad)-1],w[i].T)\n",
        "      sig=sigmoid_derivative(a[i-1])\n",
        "      ad1=np.multiply(hd1,sig)\n",
        "      hd.append(hd1)\n",
        "      ad.append(ad1)\n",
        "    wd.append(wi)\n",
        "    bd.append(bi)\n",
        "  return (wd,bd)  "
      ],
      "metadata": {
        "id": "_fYP_p056tpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(x_test,y_test,w,b):\n",
        "  a=[]\n",
        "  h=[]\n",
        "  a,h=forward_pass(x_test,w,b,a,h)\n",
        "  l=len(w)\n",
        "  ypred=np.argmax(h[l-1],axis=1)\n",
        "  #print(ypred)\n",
        "  #print(y_test)\n",
        "  count=0\n",
        "  for i in range(y_test.shape[0]):\n",
        "    if ypred[i]!=y_test[i]:\n",
        "      count=count+1\n",
        "  print((x_test.shape[0]-count)/y_test.shape[0])\n"
      ],
      "metadata": {
        "id": "C2iWR0G_dlEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradientDescent(x_train,y_train,x_test,y_test,no_of_classes,w,b,l,iter,n):\n",
        "  for i in range (iter):\n",
        "    a=[]\n",
        "    h=[]\n",
        "    wd=[]\n",
        "    bd=[]\n",
        "    ad=[]\n",
        "    hd=[]\n",
        "    a,h=forward_pass(x_train,w,b,a,h)\n",
        "    #print(a[0][:])\n",
        "    #print(a[l-1][:])\n",
        "    loss=cross_entropy(h[l-1],y_train)\n",
        "    print(\"Iteration Number: \"+str(i)+\" loss: \"+str(loss/x_train.shape[0]))\n",
        "    wd,bd=backward_pass(wd,bd,ad,hd,w,b,a,h,h[l-1],y_train,x_train)\n",
        "    for j in range (l):\n",
        "      w[j]=w[j]-n*wd[l-1-j]\n",
        "      b[j]=b[j]-n*bd[l-1-j]\n",
        "  \n",
        "\n",
        "  print(\"Test Results: \")\n",
        "  test(x_test,y_test,w,b)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "76yjTMwQssrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def oneGradientDescent(x_train,y_train,x_test,y_test,no_of_classes,w,b,l,n):\n",
        "  a=[]\n",
        "  h=[]\n",
        "  wd=[]\n",
        "  bd=[]\n",
        "  ad=[]\n",
        "  hd=[]\n",
        "  a,h=forward_pass(x_train,w,b,a,h)\n",
        "    \n",
        "  #loss=cross_entropy(h[l-1],y_train)\n",
        "  # print(\" loss: \"+str(loss/x_train.shape[0]))\n",
        "  wd,bd=backward_pass(wd,bd,ad,hd,w,b,a,h,h[l-1],y_train,x_train,no_of_classes)\n",
        "  for j in range (l):\n",
        "    w[j]=w[j]-n*wd[l-1-j]\n",
        "    b[j]=b[j]-n*bd[l-1-j]\n",
        "  \n",
        "\n",
        "  # print(\"Test Results: \")\n",
        "  # test(x_test,y_test,w,b)\n",
        "  return (w,b)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ngtTOnu32FOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchGrad(x_train,y_train,x_test,y_test,no_of_classes,w,b,l,iter,n,batchSize):\n",
        "  data=[]\n",
        "  ans=[]\n",
        "  for i in range(int(x_train.shape[0]/batchSize)):\n",
        "    batch=[]\n",
        "    batch_ans=[]\n",
        "    for j in range(i*batchSize,min((i+1)*batchSize,x_train.shape[0]),1):\n",
        "      batch.append(x_train[j])\n",
        "      batch_ans.append(y_train[j])\n",
        "    batch=np.array(batch)\n",
        "    batch_ans=np.array(batch_ans)\n",
        "    data.append(batch)\n",
        "    ans.append(batch_ans)\n",
        "  \n",
        "  #start the batch gradient descent \n",
        "  for i in range(iter):\n",
        "    h=None\n",
        "    for j in range(len(data)):\n",
        "      w,b=oneGradientDescent(data[j],ans[j],x_test,y_test,no_of_classes,w,b,l,n)\n",
        "    a=[]\n",
        "    h=[]\n",
        "    a,h=forward_pass(x_train,w,b,a,h)\n",
        "    loss=cross_entropy(h[l-1],y_train)\n",
        "    print(\"Iteration Number: \"+str(i)+\" loss: \"+str(loss/x_train.shape[0]))\n",
        "    print(\"Test Results: \")\n",
        "    test(x_test,y_test,w,b) \n",
        "\n"
      ],
      "metadata": {
        "id": "0-w5m1qOwVoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def momentumGradientDescent(x_train,y_train,x_test,y_test,no_of_classes,w,b,l,iter,n,batchSize,beta):\n",
        "  data=[]\n",
        "  ans=[]\n",
        "  for i in range(int(x_train.shape[0]/batchSize)):\n",
        "    batch=[]\n",
        "    batch_ans=[]\n",
        "    for j in range(i*batchSize,min((i+1)*batchSize,x_train.shape[0]),1):\n",
        "      batch.append(x_train[j])\n",
        "      batch_ans.append(y_train[j])\n",
        "    batch=np.array(batch)\n",
        "    batch_ans=np.array(batch_ans)\n",
        "    data.append(batch)\n",
        "    ans.append(batch_ans)\n",
        "  #batches have been made accordingly\n",
        "  moment=[]\n",
        "  momentB=[]\n",
        "  for i in range(l):\n",
        "    temp=np.zeros((w[i].shape))\n",
        "    temp2=np.zeros(b[i].shape)\n",
        "    moment.append(temp)\n",
        "    momentB.append(temp2)\n",
        "  print(len(data))\n",
        "  for i in range(iter):\n",
        "    h=None\n",
        "    for j in range(len(data)):\n",
        "      a=[]\n",
        "      h=[]\n",
        "      wd=[]\n",
        "      bd=[]\n",
        "      ad=[]\n",
        "      hd=[]\n",
        "      if j%10==0:\n",
        "        print(\"Batch Number \"+str(j))\n",
        "      a,h=forward_pass(x_train,w,b,a,h)\n",
        "    \n",
        "      #loss=cross_entropy(h[l-1],y_train)\n",
        "      # print(\" loss: \"+str(loss/x_train.shape[0]))\n",
        "      wd,bd=backward_pass(wd,bd,ad,hd,w,b,a,h,h[l-1],y_train,x_train,no_of_classes)\n",
        "      for k in range (l):\n",
        "        moment[k]=(moment[k]*beta)+wd[l-1-k]\n",
        "        momentB[k]=(momentB[k]*beta)+bd[l-1-k]\n",
        "        w[k]=w[k]-n*moment[k]\n",
        "        b[k]=b[k]-n*momentB[k]\n",
        "      \n",
        "    a=[]\n",
        "    h=[]\n",
        "    a,h=forward_pass(x_train,w,b,a,h)\n",
        "    loss=cross_entropy(h[l-1],y_train)\n",
        "    print(\"Iteration Number: \"+str(i)+\" loss: \"+str(loss/x_train.shape[0]))\n",
        "    print(\"Test Results: \")\n",
        "    test(x_test,y_test,w,b) \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w0Wz5zWOhmlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nsGradientDescent(x_train,y_train,x_test,y_test,no_of_classes,w,b,l,iter,n,batchSize,beta):\n",
        "  data=[]\n",
        "  ans=[]\n",
        "  for i in range(int(x_train.shape[0]/batchSize)):\n",
        "    batch=[]\n",
        "    batch_ans=[]\n",
        "    for j in range(i*batchSize,min((i+1)*batchSize,x_train.shape[0]),1):\n",
        "      batch.append(x_train[j])\n",
        "      batch_ans.append(y_train[j])\n",
        "    batch=np.array(batch)\n",
        "    batch_ans=np.array(batch_ans)\n",
        "    data.append(batch)\n",
        "    ans.append(batch_ans)\n",
        "  #batches have been made accordingly\n",
        "  moment=[]\n",
        "  momentB=[]\n",
        "  for i in range(l):\n",
        "    temp=np.zeros((w[i].shape))\n",
        "    temp2=np.zeros((b[i].shape))\n",
        "    moment.append(temp)\n",
        "    momentB.append(temp2)\n",
        "  print(len(data))\n",
        "  for i in range(iter):\n",
        "    h=None\n",
        "    for j in range(len(data)):\n",
        "      a=[]\n",
        "      h=[]\n",
        "      wd=[]\n",
        "      bd=[]\n",
        "      ad=[]\n",
        "      hd=[]\n",
        "      if j%10==0:\n",
        "        print(\"Batch Number \"+str(j))\n",
        "      \n",
        "    \n",
        "      #loss=cross_entropy(h[l-1],y_train)\n",
        "      # print(\" loss: \"+str(loss/x_train.shape[0]))\n",
        "      for k in range(l):\n",
        "        old=moment[k]*beta\n",
        "        oldB=momentB[k]*beta\n",
        "        w[k]=w[k]-old\n",
        "        b[k]=b[k]-oldB\n",
        "      a,h=forward_pass(x_train,w,b,a,h)  \n",
        "      wd,bd=backward_pass(wd,bd,ad,hd,w,b,a,h,h[l-1],y_train,x_train,no_of_classes)\n",
        "      for k in range (l):\n",
        "        old=moment[k]*beta\n",
        "        oldB=momentB[k]*beta\n",
        "        moment[k]=(old)+wd[l-1-k]\n",
        "        momentB[k]=oldB+bd[l-1-k]\n",
        "        w[k]=w[k]-n*moment[k]\n",
        "        b[k]=b[k]-n*momentB[k]\n",
        "      \n",
        "    a=[]\n",
        "    h=[]\n",
        "    a,h=forward_pass(x_train,w,b,a,h)\n",
        "    loss=cross_entropy(h[l-1],y_train)\n",
        "    print(\"Iteration Number: \"+str(i)+\" loss: \"+str(loss/x_train.shape[0]))\n",
        "    print(\"Test Results: Nestorov Gradient Descent :: \")\n",
        "    test(x_test,y_test,w,b) "
      ],
      "metadata": {
        "id": "HlguX4kstKPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def architecture(x_train,y_train,x_test,y_test,no_of_classes):\n",
        "\n",
        "  w=[]\n",
        "  b=[]\n",
        "  w,b=make_layer(w,b,784,128)\n",
        "  w,b=make_layer(w,b,128,128)\n",
        "  w,b=make_layer(w,b,128,128)\n",
        "  w,b=make_layer(w,b,128,10)\n",
        "  l=len(w)\n",
        "  iter=2\n",
        "  n=0.1\n",
        "  #gradientDescent(x_train,y_train,x_test,y_test,no_of_classes,w,b,l,iter,n)\n",
        "  print(\"Batch Gradient Descent: \")\n",
        "  batchGrad(x_train,y_train,x_test,y_test,no_of_classes,w,b,l,iter,n,1024)\n",
        "  print(\"Momentum Gradient Descent: \")\n",
        "  momentumGradientDescent(x_train,y_train,x_test,y_test,no_of_classes,w,b,l,iter,n,1024,0.9)\n",
        "  print(\"Starting of Nestrov Gradient Descent: \")\n",
        "  nsGradientDescent(x_train,y_train,x_test,y_test,no_of_classes,w,b,l,iter,n,1024,0.9)\n",
        "  "
      ],
      "metadata": {
        "id": "kLLtw7Us42D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "architecture(x_train,y_train,x_test,y_test,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "id": "VPjTJaW1F20e",
        "outputId": "1ae42901-6041-4c67-a385-77e65c6b0196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Gradient Descent: \n",
            "Iteration Number: 0 loss: 1.8360265145387435\n",
            "Test Results: \n",
            "0.5781\n",
            "Iteration Number: 1 loss: 1.4435367844432228\n",
            "Test Results: \n",
            "0.6633\n",
            "Momentum Gradient Descent: \n",
            "58\n",
            "Batch Number 0\n",
            "Batch Number 10\n",
            "Batch Number 20\n",
            "Batch Number 30\n",
            "Batch Number 40\n",
            "Batch Number 50\n",
            "Iteration Number: 0 loss: 0.9237465649702795\n",
            "Test Results: \n",
            "0.7591\n",
            "Batch Number 0\n",
            "Batch Number 10\n",
            "Batch Number 20\n",
            "Batch Number 30\n",
            "Batch Number 40\n",
            "Batch Number 50\n",
            "Iteration Number: 1 loss: 0.811037050019849\n",
            "Test Results: \n",
            "0.7875\n",
            "Starting of Nestrov Gradient Descent: \n",
            "58\n",
            "Batch Number 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-e7bdd83da871>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0marchitecture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-7de8ab230060>\u001b[0m in \u001b[0;36marchitecture\u001b[0;34m(x_train, y_train, x_test, y_test, no_of_classes)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mmomentumGradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mno_of_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting of Nestrov Gradient Descent: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mnsGradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mno_of_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-28e3b80dedbb>\u001b[0m in \u001b[0;36mnsGradientDescent\u001b[0;34m(x_train, y_train, x_test, y_test, no_of_classes, w, b, l, iter, n, batchSize, beta)\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch Number \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m       \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0;31m#loss=cross_entropy(h[l-1],y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-335a893d30b1>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(x, w, b, a, h)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m#use a for loop for the required functionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0ma1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mh1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (60000,128) (784,128) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def architecture(x_train,y_train,x_test,y_test,no_of_classes):\n",
        "\n",
        "  w=[]\n",
        "  b=[]\n",
        "  w,b=make_layer(w,b,784,128)\n",
        "  w,b=make_layer(w,b,128,128)\n",
        "  w,b=make_layer(w,b,128,128)\n",
        "  w,b=make_layer(w,b,128,10)\n",
        "  l=len(w)\n",
        "  iter=2\n",
        "  n=0.1\n",
        "  #gradientDescent(x_train,y_train,x_test,y_test,no_of_classes,w,b,l,iter,n)\n",
        "  #print(\"Batch Gradient Descent: \")\n",
        "  #batchGrad(x_train,y_train,x_test,y_test,no_of_classes,w,b,l,iter,n,1024)\n",
        "  #print(\"Momentum Gradient Descent: \")\n",
        "  #momentumGradientDescent(x_train,y_train,x_test,y_test,no_of_classes,w,b,l,iter,n,1024,0.9)\n",
        "  print(\"Starting of Nestrov Gradient Descent: \")\n",
        "  nsGradientDescent(x_train,y_train,x_test,y_test,no_of_classes,w,b,l,iter,n,1024,0.9)\n",
        "  "
      ],
      "metadata": {
        "id": "YdNOtR8u_uUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "architecture(x_train,y_train,x_test,y_test,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVB5oZ0RdX57",
        "outputId": "08893b76-57d3-427e-9b73-2c729c4baef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting of Nestrov Gradient Descent: \n",
            "58\n",
            "Batch Number 0\n",
            "Batch Number 10\n",
            "Batch Number 20\n",
            "Batch Number 30\n",
            "Batch Number 40\n",
            "Batch Number 50\n",
            "Iteration Number: 0 loss: 50.16974652624729\n",
            "Test Results: Nestorov Gradient Descent :: \n",
            "0.1\n",
            "Batch Number 0\n",
            "Batch Number 10\n",
            "Batch Number 20\n",
            "Batch Number 30\n",
            "Batch Number 40\n",
            "Batch Number 50\n",
            "Iteration Number: 1 loss: 34.613668183574966\n",
            "Test Results: Nestorov Gradient Descent :: \n",
            "0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YU8mYwhGdbh9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}